---
layout: post
title: Blog Post 2 - Spectral Clustering 
---

This blog post will explain a simple version of the clustering algorithm for clustering data points.

## Part A - Similarity Matrix

This blogpost will be using multiple modules to graph the data accordingly, so let's import those modules right now!

```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
import sklearn
```

In general, here's the usage for each module in this clustering algorithm:
- `numpy` will be used for manipulating arrays in a convenient way
- `sklearn` will be used for analyzing matrices 
- `matplotlib` will be used to graph the data points

Let's first create the dataset that we want to color-code based on cluster. Let's name the dataset `X` and have it contain 200 data points: 

```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```
![hw2_blue_cluster.png]({{ site.baseurl }}/images/hw2_blue_cluster.png)

Notice how there are clearly two different moon-shaped clusters on the graph! Because of this irregular shape, to color-code each moon, we'll need to execute a simple clustering algorithm.

For Part A, we first have to create the similarity matrix **A**. which is a 2D `numpy` array with shape `(n,n)` where `n` is the number of data points. In our example above, we have `n = 200`. 

To construct this matrix, we will use a positive parameter `epsilon` where `A[i,j] = 1` if `X[i]` and `X[j]` are within distance `epsilon` of each other. If not, then `A[i,j] = 0`. In addition, the diagonal entries `A[i,i] = 0` as well. To execute this, we will do the following: 

1. Create a matrix containing all the distances between points. Luckily, `sklearn.metrics` has a handy function that does just that! So, we'll use `pairwise_distances()`, which will result in an `nxn` matrix.
2.  We will then use a boolean mask to denote which values should be `0` and which ones should be `1`. In this case, we can directly compare the matrix created with `pairwise_distances()` with the value of `epsilon` and multiply by `1` to get the comparison, because `True` has a value of `1` and `False` has a value of `0`.
3. We will manually fill the diagonal with `0`s using `np.fill_diagonal()` since its distances will be `0` which is less than `epsilon`, so it would result in being `1` instead of the correct `0`. 

In this example, let `epsilon = 0.4`. All together, the code should look like so: 

```python
epsilon = 0.4

#create a matrix containing 1 if the distance is less than epsilon(size nxn because X is nxn)
A = 1*(pairwise_distances(X) < epsilon)

#fill the diagonal with 0s
np.fill_diagonal(A,0)

#see what A looks like
print(A)
```
```
[[0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 1 0]
 ...
 [0 0 0 ... 0 1 1]
 [0 0 1 ... 1 0 1]
 [0 0 0 ... 1 1 0]]
```
{::options parse_block_html="true" /}
<div class="got-help">
Originally, I created the similarity matrix **A** using two boolean masks to account for each case. While the code worked, it was not that efficient or clear.
One of my peer reviewers for this assignment showed me how to synthesize all my boolean masks into one line of code by directly `comparing pairwise_distances(X)` to `epsilon`, which makes the code more efficient and readable! 
</div>
{::options parse_block_html="false" /}

Our matrix looks great! Filled with only 0s and 1s. 

## Part B - Binary Norm Cut Objective

Now that **A** contains information about which points are near which points, we will now try to cluster the data points accordingly. First, let's define some mathematical expressions: 

Let's denote $$C_0$$ and $$C_1$$ as the two clusters of the data points in our graph. For this clustering algorithm, let's assume that every data point is in exactly one cluster. `y` will denote our cluster membership. This means that the value of `y[i]` will denote which cluster the point `i` (which then means the row $$i$$ of $$\mathbf{A}$$) is an element of.  

Now, to cluster the data points, we will compute the *binary norm cut objective* of the matrix **A**, which is the following: 

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

This function uses two other functions:
- $$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$ is the *cut* of the clusters $$C_0$$ and $$C_1$$. 
- $$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$, where $$d_i = \sum_{j = 1}^n a_{ij}$$ is the $$i$$th row-sum of  **A**. This expression represents the volume of $$C_0$$, so we would have to do the same calculation for $$C_1$$. Essentially, the volume of a cluster represents the size of the cluster. 

With these formulas, we say that two clusters are clearly partitioned when the norm cut objective is small. However, to compute the norm cut objective, we'll need to compute a few other functions first.

Let's first focus on the cut term. Based on the formula, we can compute this term by adding up the entries `A[i,j]` for each pair of points `(i,j)` in different clusters. Since `y[i]` signifies which cluster the data point is in, we can write the code for a `cut()` function, which takes in the similarity matrix `A` and the cluster membership `y` and outputs the value of the cut term, like so:

```python
def cut(A,y):
    """
    @param A: similarity matrix
    @param y: cluster membership vector
    @return the cut term
    """
    length, width = A.shape
    cut = 0
    
    #loop through each element in A and sum up the points in different clusters as defined above
    for i in range(length):
        for j in range(width):
            if y[i] != y[j]:
                cut += A[i][j]
    
    return cut
```

Let's test this out with our similarity matrix **A** and cluster membership vector `y`. To compare, the cut object should be much smaller to a randomally generated vector. So, let's compare these two values like so: 

```python
cut_real = cut(A,y) 
rand_v = np.random.randint(0,2, size = (n,)) 
cut_rand = cut(A,rand_v)

print(cut_real)
print(cut_rand)
```
```
26
2184
```

Our correct cut object is much smaller, which means we're on the right track!

Next, let's compute the volume term. To do so, we want the sum of row `i` if `y[i] == 0` for $$C_0$$ and `y[i] == 1` for $$C_1$$. We can compute this very easily and efficiently using `np.sum()`! With this function, if we set the parameter `axis = 1`, it will sum the matrix **A** by row. Then, we'll use boolean masking again to gather the values we want based on the cluster membership. So, let's create a `vols()` function that returns the volumes of each cluster as a tuple:

```python
def vols(A,y):
    """
    @param A: similarity matrix
    @param y: cluster membership vector
    @return the volumes of each cluster as a tuple
    """
    #sum each row, select the ones based on each cluster, and sum together
    vol0 = np.sum(A, axis = 1)[y == 0].sum()
    vol1 = np.sum(A, axis = 1)[y == 1].sum()
    
    return (vol0, vol1)
```
{::options parse_block_html="true" /}
<div class="got-help">
For my first draft, I constructed the volumes using list comprehension like so: 
```python
vol0 = sum([sum(A[i]) for i in range(length) if y[i] == 0])
vol1 = sum([sum(A[i]) for i in range(length) if y[i] == 1])
```
One of my peer reviewers reminded me that `numpy` has a variety of functions that can compute the sum of rows without using list comprehension. It makes the code a lot easier to read, and takes advantage of the `numpy` module that we use throughout this algorithm!
</div>
{::options parse_block_html="false" /}

Now that we have both `vols()` and `cut()` defined, we can create a `normcut()` function that uses these two functions to compute the norm cut objective! Based on the formula, `normcut()` will be like so:

```python
def normcut(A,y):
    v0, v1 = vols(A,y)
    return cut(A,y)*((1/v0) + (1/v1))
```

Now, let's test this function! To compare, the norm cut of the correct labels should be significantly smaller than the norm cut of a randomly generated vector: 

```python
print(normcut(A,y))
print(normcut(A, rand_v))
```
```
0.02303682466323045
2.0650268054241563
```

Notice that 0.023 is a lot smaller than 2.065, so we're on the right track!

## Part C - Another Approach at Norm Cut

Our approach works perfectly, however, using linear algebra, we can also derive the following vector **z** such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$

Using this vector **z** and some linear algebra, we can compute the norm cut like so: 

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

where $$\mathbf{D}$$ is the diagonal matrix where its $$i$$th diagonal entries are the $$i$$th row-sum of **A**.

Note that `z[i]` will be greater than 0 if the point is in $$C_0$$ and will be less than 0 if the point is in $$C_1$$. With this new approach, let's create a new function `transform()` which will return the appropriate **z** vector based on the formula above:

```python
#create z based on the formula
def transform(A,y):
    #get the volumes of the clusters
    v0, v1 = vols(A,y)
    #first fill z with 1/v0
    z = (1/v0)*np.ones(len(y))
    
    #use a boolean mask to see which values should be -1/v1
    z[y == 1] = -1/v1
    
    return z
```

To test our function, let's compute the matrix product and see how it compares to our original norm cut value from Part B. Let's also check that $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$, where $$\mathbb{1}$$ is the vector of `n` ones, which should be true if our **z** is correct. This essentially says that **z** contains around the same number of positive and negative entries. We'll use `np.isclose()` to determine if the two norm cut calculations are about equal, because slight differences due to rounding could result in `False` if we were to use `==`.

```python
#create vector z and its transpose
z = transform(A,y)
z_T = z.T

#create D by first making a nxn array with the diagonal entries as the sum of the rows of A
D = np.zeros(shape = (n,n))
np.fill_diagonal(D, A.sum(axis = 1))

#based on the matrix multiplication formula
norm1 = 2*(z_T@(D-A)@z)/(z_T@D@z)
#from Part B
norm2 = normcut(A,y)

print(np.isclose(norm1, norm2))

print(z_T@D@(np.ones(n)))
```
```
True
0.0
```

Looks like everything checks out!


## Part D - Minimizing a Function

The matrix multiplication above can be optimized by substituing **z** for its orthogonal complement relative to $$\mathbf{D}\mathbb{1}$$ and optimizing this function in relation to **z**. 

The following code defines the orthogonal complement calculation:

```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```

For this part, let's use the `minimize` function from `scipy.optimize` to minimize this function `orth_obj()` like so:

```python
import scipy
z_ = scipy.optimize.minimize(orth_obj, z)
```

Now, `z_` is our minimized **z** vector. 

## Part E - Graphing Clusters with the Minimized Vector

Let's plot the moon-shaped clusters again using this optimized `z_` vector to dictate which color each data point is. In theory, `z_[i]` should be greater than 0 when it is in $$C_0$$, but since this isn't the best optimization problem, we'll make the bottom threshold a small negative number to accommodate for this error. In this instance, I chose `-0.0015`:

```python
plt.scatter(X[:,0], X[:,1], c = [z_.x < -0.0015])
plt.show()
```
![hw2_firstcolorplot.png]({{ site.baseurl }}/images/hw2_firstcolorplot.png)

Notice how we came close, but it isn't perfect. The left tip of the bottom moon-shaped cluster is purple instead of yellow... So let's make the optimization a little better!

## Part F - Laplacian Matrix

Since our original calculation isn't the best, nor is it the most efficient, let's try to optimize it more! Like before, we want to minimize this function: 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

with respect to **z**. 

The Rayleigh-Ritz Theorem states that the minimized **z** is also the solution with smallest eigenvalue of the following equation:

$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

Using some linear algebra, we can find that the eigenvector associated to the second-smallest eigenvalue of the matrix $$\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$ is our minimized **z** that we want.

So, with this math in mind, let's compute the second-smalest eigenvalue of $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$. This matrix **L** is often called the (normalized) *Laplacian* matrix of the similarity matrix $$\mathbf{A}$$.

To do this, let's first compute **L**, and use `np.linalg.eig()` to calculate the eigenvalues and eigenvectors associated with **L**. We will then order the eigenvalues in order from least to greatest, and choose the eigenvector associated to the second-smallest eigenvalue, which will be at index 1 in our list. All together, the code looks like so:

```python
#compute the inverse of D from Part C
D_inv = np.linalg.inv(D)
#compute L
L = D_inv@(D-A)

#Lam = eigenvalues, U = eigenvectors 
Lam, U = np.linalg.eig(L)
#sort in ascending order by eigenvalue 
ix = Lam.argsort()
Lam,U = Lam[ix], U[:,ix]

z_eig = U[:,1]
```

Now that we have this eigenvector, we can plot our moon-shaped dataset, using `z_eig` for the colors instead of `z_`. Note that the data point still belongs in $$C_0$$ if `z_eig[i] > 0` and vice versa if the data point belongs in $$C_1$$.

```python
plt.scatter(X[:,0], X[:,1], c = [z_eig < 0])
plt.show()
```
![hw2_colorploteig.png]({{ site.baseurl }}/images/hw2_colorploteig.png)

Looking a lot better!

## Part G - A Synthesized Version

Now, let's put all of this together into one function! So, let's create the function `spectral_clustering(X, epsilon)` where `X` is the data and `epsilon` is the distance threshold. With this data, we need to achieve the following:

1. Compute the similarity matrix **A**. 
2. Compute the Laplacian matrix **L**. 
3. Find the eigenvector associated with the second-smallest eigenvalue of **L**. 
4. Return labels for coloring based on this eigenvector. Note that this can be a `numpy` array of booleans, which indicate which data points should be which color.

This is all code we've written and gone through before, so let's put it all together:

{::options parse_block_html="true" /}
<div class="gave-help">
Something that I found particularly helpful was consistent commenting throughout functions! Especially with large and dense functions like `spectral_clustering()`, I thought it was very helpful to break the code up into separate chunks, where each chunk executed a specific task. I then commented each section so the reader can understand the function in a step-by-step way. I personally found this to be the clearest way to explain the function's construction, and suggested that my peers also add meaningful and concise comments for their more complex functions, like `spectral_clustering()`!
</div>
{::options parse_block_html="false" /}

```python
def spectral_clustering(X, epsilon): 
    """
    @param X: nxn matrix of data
    @param epsilon: positive number, represents minimum distance
    @return array of binary labels which indicate if a data point i is in group 0 or group 1
    """
    
    """construct the similarity matrix using boolean masks"""
    A = 1*(pairwise_distances(X) < epsilon)
    np.fill_diagonal(A,0)
    
    """create the Laplacian matrix using the defined diagonal matrix D"""
    D = np.zeros(shape = A.shape)
    np.fill_diagonal(D, A.sum(axis = 1))
    L = (np.linalg.inv(D))@(D-A)
    
    """compute the second-smallest eigenvalue"""
    Lam, U = np.linalg.eig(L)
    ix = Lam.argsort()
    Lam,U = Lam[ix], U[:,ix]

    """return the eigenvector associated with the second-smallest eigenvalue"""
    return [U[:,1] < 0]
```
{::options parse_block_html="true" /}
<div class="gave-help">
I thought that returning a boolean vector for `spectral_clustering()` was a nice way of allowing the user to use this function directly into the `c` parameter of `plt.scatter`, which denotes the coloring of the data points. This way, I could take advantage of the fact that `True` is 1 and `False` is 0, so each point's color was accounted for with this boolean vector! A few of my classmates did not notice this, so I suggested this to them so they could keep their code simpler. 
</div>
{::options parse_block_html="false" /}

Using this synthesized version, let's plot the moon-shaped data again!

```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
plt.show()
```
![hw2_colorplotsynth.png]({{ site.baseurl }}/images/hw2_colorplotsynth.png)

## Part H - Some Other Moon-Shaped Graphs

Let's see how our function does against other data sets! We'll continue using `make_moons`, but let's use 1000 data points instead this time:

```python
#1000 points
X2, y2 = datasets.make_moons(n_samples=1000, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X2[:,0], X2[:,1], c = spectral_clustering(X2, 0.5))
```
![hw2_colorplot1000.png]({{ site.baseurl }}/images/hw2_colorplot1000.png)

Looks good! Just the tips of the moons closest to the other cluster are a little discolored, but this is minor. 

Now, let's see how adding noise to 1000 data points will change the graph: 

```python
#1000 points, 0.10 noise
X1, y1 = datasets.make_moons(n_samples=1000, shuffle=True, noise=0.10, random_state=None)
plt.scatter(X1[:,0], X1[:,1], c = spectral_clustering(X1, 0.4))
```
![hw2_colorplot1000noise.png]({{ site.baseurl }}/images/hw2_colorplot1000noise.png)

Also looking good! Again, the tips of the moons closest to the other cluster are a little discolored, but this is minor again. 

## Part I - Bull's Eye Graph

Now, let's try our spectral clustering function on a bull's eye function!

This is what the graph will look like with no coloring:

```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```
![hw2_bluebull.png]({{ site.baseurl }}/images/hw2_bluebull.png)

Let's test different values of `epsilon` to see which one will result in the best coloring for each ring. 

First, let's start larger. Let's have `epsilon = 0.6` and see what happens: 

```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.6))
```
![hw2_bull0.6.png]({{ site.baseurl }}/images/hw2_bull0.6.png)

Oh no! We chose an epsilon that was too large. Let's lower it a little to `0.5`:

```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.5))
```
![hw2_bull0.5.png]({{ site.baseurl }}/images/hw2_bull0.5.png)

Perfect! Just for fun, let's see what an even lower epsilon value looks like. Here, I set `epsilon = 0.2`: 

```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.2))
```
![hw2_bull0.2.png]({{ site.baseurl }}/images/hw2_bull0.2.png)

Wow! So, it seems that `epsilon` around 0.5 will do it for the bull's eye graph. Now, we have successfully created a spectral clustering function for graphs with two clusters!
