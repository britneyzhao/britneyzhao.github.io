---
layout: post
title: Blog Post 3 - Analyzing Fake News with Tensorflow
---

This blog post will explain how create a model to identify fake news using Tensorflow. 

## Acquire Training Data

First, this model uses lots of modules, so let's import them and explain their use:

```python
import numpy as np
import pandas as pd
import tensorflow as tf
import re #regular expressions
import string

from tensorflow.keras import layers #creating layers
from tensorflow.keras import losses #loss functions
from tensorflow import keras

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

# for embedding viz
import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"
```

Overall, we have a little bit to unpack here, but we'll mainly be using the following modules for various reasons: 
- `numpy`: to manipulate arrays in a convenient way
- `pandas`: to create dataframes
- `tensorflow`: to implement machine learning techniques
    - `layers` is for creating layers in our algorithm
    - `losses` is for loss functions
- `re`: for creating regular expressions for string recognition 
- `string`: for manipulating strings
- `plotly`: for ploting visualizations of our data

With these modules, let's first acquire our training data! For this project, we want to ultimately create a model that can accurately determine if a news article is fake or not. To do so, we will need some data to train our model. This training data contains information about the title of the article, text from the article, and whether it is fake or not. Let's use `pd.read_csv()` to first obtain this data: 

```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"

df = pd.read_csv(train_url)
df
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>22444</th>
      <td>10709</td>
      <td>ALARMING: NSA Refuses to Release Clinton-Lynch...</td>
      <td>If Clinton and Lynch just talked about grandki...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22445</th>
      <td>8731</td>
      <td>Can Pence's vow not to sling mud survive a Tru...</td>
      <td>() - In 1990, during a close and bitter congre...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22446</th>
      <td>4733</td>
      <td>Watch Trump Campaign Try To Spin Their Way Ou...</td>
      <td>A new ad by the Hillary Clinton SuperPac Prior...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22447</th>
      <td>3993</td>
      <td>Trump celebrates first 100 days as president, ...</td>
      <td>HARRISBURG, Pa.U.S. President Donald Trump hit...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22448</th>
      <td>12896</td>
      <td>TRUMP SUPPORTERS REACT TO DEBATE: “Clinton New...</td>
      <td>MELBOURNE, FL is a town with a population of 7...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>22449 rows × 4 columns</p>
</div>

In this dataframe, the `fake` column will have `0` represent if an article is true, and `1` if the article contains fake news. Now, we have lots of data to train our model with! 

## Make a Dataset

Now that we have the data that we want to analyze, we'll need to first create a dataset that Tensorflow will be able to interpret. To do so, let's create a function `make_dataset()` that will both clean our data and compile it into this dataset. 

For cleaning our data, we want to remove all *stopwords* from it, or words that are usually uninformative, such as "the" or "and." To recognize these words, we'll use the list of stopwords from the module `ntlk`. Essentially, we want to go through every word in the title and text of each article, remove it if it's a stopword, then create our dataset with these cleaned data. 

Let's first create a `remove_stopwords` function that will remove all the stopwords from a list of strings like so:

```python
#import list of stopwords
from nltk.corpus import stopwords 
stop = stopwords.words('english')

def remove_stopwords(l):
  #loop through each word, keep the ones that aren't stop words
  l = l.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  return l
```
{::options parse_block_html="true" /}
<div class="got-help">
I originally included this lambda function in `make_dataset()` to edit both `df['title']` and `df['text']`. However, my peer noted that I could create a helper function that reduced the amount of repeated code. The lambda function is not as intuitive, so my peer also suggested I add a small explanation as to what exactly is happening in it. This way, it's a lot more clear exactly how to remove stopwords from any list of strings! 
</div>
{::options parse_block_html="false" /}

We want to have the model observe the title and text of the article, then predict if the article has fake information in it or not. Therefore, we want to create a dataset in the form like so: `(input, output)`. We'll use a tuple of dictionaries to do this, so the "title" of an article is associated with the `title` column of the dataframe `df` from before. All together, we can create the dataset and remove the stopwords like so: 

```python
def make_dataset(df):
  #remove stopwords
  df['title'] = remove_stopwords(df['title'])
  df['text'] = remove_stopwords(df['text'])

  #create dataset
  data = tf.data.Dataset.from_tensor_slices(
      (
        {
            "title" : df[['title']],
            "text" : df[['text']]
        },
       {
            "fake" : df[['fake']]   
       }
      )
  )
  data.batch(100)
  return data
```

Great! Now we have a function that takes in a `pandas` dataframe and returns a Tensorflow dataset that we can use for our machine learning algorithm. With this function, let's create our dataset for this example: 

```python
data = make_dataset(df)
```

Now that we have the dataset, we want to split our data into two different parts: training and validation. Majority of the data will be used for training, since that's what we need to create a proper model! The validation data will be used as new pieces of data that the model hasn't seen to see how it responds at a certain round of training. For this example, we'll have 80% training data and 20% validation data overall. Since our dataset is quite large, we'll compile it into batches of 20 to make it more efficient. All together, we can split it up like so: 

```python
train_size = int(0.8*len(data))

train = data.take(train_size).batch(20)
val = data.skip(train_size).batch(20)
```

We now have data to create a model with! 

## Model Creation

Let's create three different models to see which analyzes the articles the best! One will use the article's title, one will use the article's text, and our final one will use both the title and text. To do so, we'll use the following process:
1. Create input that Tensorflow can interpret
2. Process the input information and vectorize it so that the algorithm can understand it 
3. Create each layer of the fake news identifier, which will ultimately become our model
4. Compile the model 
5. Train the model using our data from before

With this process in mind, let's create our first model, using only the article's title!

### Model 1 - Article Title

Let's follow the process we outlined above! To do so, let's first create input that Tensorflow understands. To do so, we'll use the function `keras.Input()` from Tensorflow. We can use it like so, indicating the shape, the data, and the datatype of the input: 

```python
title_input = keras.Input(
    shape = (1,),
    name = "title",
    dtype = "string"
)
```

Next, we'll need to process this information and vectorize it in a way that an algorithm can understand. Computers can understand numbers very clearly, so let's vectorize the strings so that each string is associated with a number instead. First, we'll need to standardize the strings. This involves making it all lowercase and taking out the punctuation. So, let's create a function executing just that: 

```python
def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 
```

With this function, we can create a layer in our algorithm just for vectorizing the input data. We'll use `TextVectorization()` from Tensorflow to do so. This layer from Tensorflow interprets text into numbers that will be understandable for other layers. For our purposes, we will only consider the 2000 most popular words within our dataset, and not specifically analyze every word, which is represented through the variable `size_vocabulary`. Afterwards, we'll want to analyze the entire title of the article, and not just the first word, so we'll need to use `adapt()` and `map()` to execute this. All together, creating this layer will look like so: 

```python
size_vocabulary = 2000

vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

#get the whole title out with lambda function
vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
```

This was the hardest part! With that out of the way, we can begin adding more layers. The other layers we'll use are the following: 
1. `Embedding` to see what words are associated with fake news. We'll use 10 dimensions of words to analyze. 
2. `Dropout` to deal with overfitting. This ensures the model isn't overly complex.
3. `GlobalAveragePooling1D` to take the average over the entire title. This makes our original 10 dimensional word to a 1D vector
4. `Dense` is the most basic layer, which will execute our basic machine learning algorithm!

For both analyzing the article title and article text, we'll need an Embedding layer that analyzes each word in the same way. So, let's first create an embedding layer for both models:

```python
size_vocabulary = 2000
embedding = layers.Embedding(size_vocabulary, 10, name = "embedding")
```

{::options parse_block_html="true" /}
<div class="got-help">
Originally, I created two different `Embedding` layers for each of my models: one for article title and one for article text. However, one of my peers noted that I could create one embedding layer that both the title data and the text data could point to. This simplified the rest of this project, as I originally had to visualize two different embedding layers that were practically the same thing. This way, I only have to involve one! 
</div>
{::options parse_block_html="false" /}

We'll use all of these together to get all the features like so: 

```python
title_features = vectorize_layer(title_input)
title_features = embedding(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)

output_title = layers.Dense(2, name = "fake")(title_features)
```

With all of our layers, we can now construct our model! Let's use the function `keras.Model()` to do so:

```python
model_title = keras.Model(
    inputs = title_input,
    outputs = output_title
)
```

After constructing the model itself, we need to compile and train the model to create the fake news identifier! We'll use `compile()` and `fit()` to execute this. We'll use 50 epochs for this model, which means that we'll compile through the training data 50 times to train our model. It will look like so: 

```python
#compile the model
model_title.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics = ['accuracy'])
              
#train the model
history = model_title.fit(train, validation_data = val, epochs = 50)
```
```
Epoch 1/50

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:591: UserWarning:

Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.

898/898 [==============================] - 5s 5ms/step - loss: 0.6408 - accuracy: 0.6478 - val_loss: 0.4064 - val_accuracy: 0.9312
Epoch 2/50
898/898 [==============================] - 3s 4ms/step - loss: 0.2068 - accuracy: 0.9451 - val_loss: 0.1127 - val_accuracy: 0.9670
Epoch 3/50
898/898 [==============================] - 4s 4ms/step - loss: 0.1034 - accuracy: 0.9668 - val_loss: 0.0801 - val_accuracy: 0.9768
.....
Epoch 48/50
898/898 [==============================] - 4s 4ms/step - loss: 0.0209 - accuracy: 0.9935 - val_loss: 0.1597 - val_accuracy: 0.9506
Epoch 49/50
898/898 [==============================] - 3s 4ms/step - loss: 0.0208 - accuracy: 0.9921 - val_loss: 0.1998 - val_accuracy: 0.9385
Epoch 50/50
898/898 [==============================] - 4s 4ms/step - loss: 0.0204 - accuracy: 0.9927 - val_loss: 0.1536 - val_accuracy: 0.9521
```

The output actually shows each level, but I've shortened it here to make it easier to interpret at first glance. We want to look at our accuracy and loss levels specifically. In a correct model, the loss fuction should decrease over epochs as it represents how optimized how function is, and accuracy should increase which means the model is improving with the dataset. Over time, we see our loss value decrease and our accuracy incraese, which means we're on the right track! We can see that during our last epoch, we obtain a validation accuracy of 0.9521, or 95.21%, which means our model is pretty good! This accuracy denotes the accuracy of our model using the validation data we partitioned off earlier. Now, our title model is complete! 

### Model 2 - Article's Text

We'll be following the same process for analyzing the article's text for a model! So, lets create our input using the same function as Model 1:

```python
text_input = keras.Input(
    shape = (1,),
    name = "text",
    dtype = "string"
)
```

We'll also be using the same vectorization and the same layers as before for the same reasons. Ultimately, we're analyzing text, so the overall process is very similar. We'll use the same `standardization()` function and create all the layers of the model like so: 

```python
vectorize_layer.adapt(train.map(lambda x, y: x["text"]))

text_features = vectorize_layer(text_input)
text_features = embedding(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)

output_text = layers.Dense(2, name = "fake")(text_features)
```

Now that we have our layers, we can now construct our model with `keras.Model()` like so: 

```python
model_text = keras.Model(
    inputs = text_input,
    outputs = output_text
)
```

With this construction, let's compile and train our model, like we did for Model 1: 

```python
#compile the model
model_text.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics = ['accuracy'])
              
#train the model
history_text = model_text.fit(train, validation_data = val, epochs = 50)
```
```
Epoch 1/50

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:591: UserWarning:

Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.

898/898 [==============================] - 6s 6ms/step - loss: 0.5238 - accuracy: 0.7465 - val_loss: 0.2281 - val_accuracy: 0.9532
Epoch 2/50
898/898 [==============================] - 5s 5ms/step - loss: 0.2156 - accuracy: 0.9260 - val_loss: 0.1397 - val_accuracy: 0.9650
Epoch 3/50
898/898 [==============================] - 5s 5ms/step - loss: 0.1557 - accuracy: 0.9495 - val_loss: 0.1137 - val_accuracy: 0.9690
.....
Epoch 48/50
898/898 [==============================] - 4s 5ms/step - loss: 0.0184 - accuracy: 0.9937 - val_loss: 0.0986 - val_accuracy: 0.9788
Epoch 49/50
898/898 [==============================] - 5s 5ms/step - loss: 0.0191 - accuracy: 0.9927 - val_loss: 0.0941 - val_accuracy: 0.9806
Epoch 50/50
898/898 [==============================] - 5s 5ms/step - loss: 0.0171 - accuracy: 0.9942 - val_loss: 0.0980 - val_accuracy: 0.9795
```

Again, let's look at our accuracy and loss function over time! We can see our loss function overall decreases, and our accuracy increases, which is great! After the last epoch, we have a validation accuracy of 0.9795, or 97.95%, which is good improvement from our first model! Now, our text model is complete!

### Model 3 - Title and Text

For this final model, we'll be analyzing the article's title and text to determine if it contains false content. Since we've already created the models for analyzing title and text separately, this model will be quite simple! We just need to combine these two models, add a few `Dense` layers, and we'll be good to go! To combine the two models, we'll use `layers.concatenate()`, then create our `Dense` layers like the previous examples like so:

```python
main = layers.concatenate([title_features, text_features], axis = 1)
main = layers.Dense(32, activation = 'relu')(main)
output = layers.Dense(2, name = "fake")(main)
```
{::options parse_block_html="true" /}
<div class="gave-help">
For creating the model analyzing both article title and text, I found that the previous models did exactly what I had originally planned for this combined model. Therefore, I simply concatenated them together to create this model involving title and text. A few of my peers created an entirely new model for this portion of the project, so I suggested that they instead use the models they already created to make this final model!
</div>
{::options parse_block_html="false" /}

With this, our layers are all complete! Now, we can simply create our model like before: 

```python
model = keras.Model(
    inputs = [title_input, text_input],
    outputs = output
)
```

And that's it! To see exactly what our model is doing, let's use a handy function called `keras.utils.plot_model()`, which will show us the exact relationship between our input, the output, and all of the layers: 

```python
keras.utils.plot_model(model)
```
![hw3-flow.png]({{ site.baseurl }}/images/hw3-flow.png)

{::options parse_block_html="true" /}
<div class="gave-help">
I found this flowchart graphic from lecture to be very helpful when understanding how to interpret models with multiple inputs, so I decided to include for this project as well! I like how it clearly showed what our model was going through, which can get complicated, especially for beginners at Tensorflow. A few of my peers did not include this visual, so I suggested they add it since it requires only one line of code and creates a very informative representation of the model! 
</div>
{::options parse_block_html="false" /}

This diagram clearly shows how we end up reaching our output goal with the two inputs! Now, we can compile and train our model like before: 

```python
#compile the model
model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics = ['accuracy'])

#train the model
history = model.fit(train, validation_data = val, epochs = 50)
```
```
Epoch 1/50
898/898 [==============================] - 7s 7ms/step - loss: 0.0438 - accuracy: 0.9881 - val_loss: 0.0849 - val_accuracy: 0.9788
Epoch 2/50
898/898 [==============================] - 7s 7ms/step - loss: 0.0176 - accuracy: 0.9945 - val_loss: 0.0891 - val_accuracy: 0.9800
Epoch 3/50
898/898 [==============================] - 7s 7ms/step - loss: 0.0167 - accuracy: 0.9947 - val_loss: 0.0965 - val_accuracy: 0.9800
.....
Epoch 48/50
898/898 [==============================] - 6s 7ms/step - loss: 0.0049 - accuracy: 0.9982 - val_loss: 0.1327 - val_accuracy: 0.9811
Epoch 49/50
898/898 [==============================] - 6s 7ms/step - loss: 0.0074 - accuracy: 0.9972 - val_loss: 0.1152 - val_accuracy: 0.9815
Epoch 50/50
898/898 [==============================] - 6s 7ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.1297 - val_accuracy: 0.9811
```
With this model, let's analyze our loss function and accuracy again. Our loss function decreases over time, and our validation accuracy reaches 0.9811 or 98.11% after our last epoch. This is great! Our model works wonderfully in identifying fake news in articles. We have now successfuly created three different models for identifying fake news. 

Comparing the performance between all three methods, we can see that **Model 3**, which analyzes both the title and the text of an article, is best at detecting fake news. Second place would be Model 2, which analyzes only the text of an article, and last place would be Model 3, which analyzes the title of an article. Therefore, algorithms detecting fake news should use both resources if they are available! However, all three models achieved an accuracy rate of at least 95%, which is pretty good overall! So, if only the title of an article is available, it could still be a pretty accurate model. But, in an ideal case, we should use **Model 3** with both title and text for the best results!

## Model Evaluation

Now that we've determined that Model 3 is our best model, let's test it on some new data! For this, let's read a new csv file, use `make_dataset()` to make it a data type that Tensorflow can understand, and evaluate how accurate our model is. To evaluate, we'll use the `evaluate` function from Tensorflow:

```python
#model evaluation

test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
df_test = pd.read_csv(test_url)
data_test = make_dataset(df_test)

model.evaluate(data_test)
```
```
22449/22449 [==============================] - 35s 2ms/step - loss: 0.1554 - accuracy: 0.9786
[0.1553690880537033, 0.9785736799240112]
```

This output means that our loss function has a value of 0.1554 and our accuracy is 0.9786, or 97.86%, which is excellent! Our model works on a totally new dataset. With this model, we'd detect fake news around 98% of the time. 

## Embedding Visualization

In our model, we created an embedding layer for all of our models. This embedding creates an association between the words and their category of truth: either fake or not. Because of this, we can analyze how each word relates to being fake or not. In other words, we can see if some words appear more frequently in fake news articles than others. 

To execute this analysis, we'll need to get the weights from the embedding layer of our model, vectorize the vocabulary, then transform the weights to be two-dimensional so we can plot the results on a coordinate plane. 

To get the weights and vectorize the vocabulary, we'll use functions from Tensorflow like so:

```python
weights = model.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer
vocab = vectorize_layer.get_vocabulary()   
```

Now, we'll need to transform the weights into a two-dimensional vector, so we can easily plot it. To do this, we'll use `PCA` from `sklearn.decomposition`, which is a function that does just that! 

```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)
```

Now, our weights make sense in the coordinate plane, we can create a dataframe that compares the weights to the vectorized vocabulary like so:

```python
embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
```

Now, we can plot this dataframe using `plotly`:

```python
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df))),
                 size_max = 2,
                 hover_name = "word")

fig.show()
```
{% include embed.html %}

Here, we can see the graph generally goes either to the left or the right. On the left, we have words like "daily" and "screenshot," whereas the right has words like "trumps" and "obamas." From this, we can see that the more politically-driven articles tend to have more false information within them, whereas articles with more neutral titles for daily news consumption tend to have little to no fake news. 

Congrats! We've now successfully created a model to detect fake news based on an article's title and text, while also analyzing potential words that occur more frequently in articles containing false information through visualizing the embedding layer. 
