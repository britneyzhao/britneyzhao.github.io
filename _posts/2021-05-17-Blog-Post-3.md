---
layout: post
title: Blog Post 3 - Analyzing Fake News with Tensorflow
---

This blog post will explain how create a model to identify fake news using Tensorflow. 

## Acquire Training Data

First, this model uses lots of modules, so let's import them and explain their use:

```python
import numpy as np
import pandas as pd
import tensorflow as tf
import re #regular expressions
import string

from tensorflow.keras import layers #creating layers
from tensorflow.keras import losses #loss functions
from tensorflow import keras

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

# for embedding viz
import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"
```

Overall, we have a little bit to unpack here, but we'll mainly be using the following modules for various reasons: 
- `numpy`: to manipulate arrays in a convenient way
- `pandas`: to create dataframes
- `tensorflow`: to implement machine learning techniques
    - `layers` is for creating layers in our algorithm
    - `losses` is for loss functions
- `re`: for creating regular expressions for string recognition 
- `string`: for manipulating strings
- `plotly`: for ploting visualizations of our data

With these modules, let's first acquire our training data! For this project, we want to ultimately create a model that can accurately determine if a news article is fake or not. To do so, we will need some data to train our model. This training data contains information about the title of the article, text from the article, and whether it is fake or not. Let's use `pd.read_csv()` to first obtain this data: 

```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"

df = pd.read_csv(train_url)
df
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>22444</th>
      <td>10709</td>
      <td>ALARMING: NSA Refuses to Release Clinton-Lynch...</td>
      <td>If Clinton and Lynch just talked about grandki...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22445</th>
      <td>8731</td>
      <td>Can Pence's vow not to sling mud survive a Tru...</td>
      <td>() - In 1990, during a close and bitter congre...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22446</th>
      <td>4733</td>
      <td>Watch Trump Campaign Try To Spin Their Way Ou...</td>
      <td>A new ad by the Hillary Clinton SuperPac Prior...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22447</th>
      <td>3993</td>
      <td>Trump celebrates first 100 days as president, ...</td>
      <td>HARRISBURG, Pa.U.S. President Donald Trump hit...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22448</th>
      <td>12896</td>
      <td>TRUMP SUPPORTERS REACT TO DEBATE: “Clinton New...</td>
      <td>MELBOURNE, FL is a town with a population of 7...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>22449 rows × 4 columns</p>
</div>

In this dataframe, the `fake` column will have `0` represent if an article is true, and `1` if the article contains fake news. Now, we have lots of data to train our model with! 

## Make a Dataset

Now that we have the data that we want to analyze, we'll need to first create a dataset that Tensorflow will be able to interpret. To do so, let's create a function `make_dataset()` that will both clean our data and compile it into this dataset. 

For cleaning our data, we want to remove all *stopwords* from it, or words that are usually uninformative, such as "the" or "and." To recognize these words, we'll use the list of stopwords from the module `ntlk`. Essentially, we want to go through every word in the title and text of each article, remove it if it's a stopword, then create our dataset with these cleaned data. 

We want to have the model observe the title and text of the article, then predict if the article has fake information in it or not. Therefore, we want to create a dataset in the form like so: `(input, output)`. We'll use a tuple of dictionaries to do this, so the "title" of an article is associated with the `title` column of the dataframe `df` from before. All together, the code should look like so: 

```python
#import list of stopwords
import nltk
nltk.download('stopwords')
stop = stopwords.words('english')

def make_dataset(df):
  #remove stopwords
  df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

  #create dataset
  data = tf.data.Dataset.from_tensor_slices(
      (
        {
            "title" : df[['title']],
            "text" : df[['text']]
        },
       {
            "fake" : df[['fake']]   
       }
      )
  )
  data.batch(100)
  return data
```

Great! Now we have a function that takes in a `pandas` dataframe and returns a Tensorflow dataset that we can use for our machine learning algorithm. With this function, let's create our dataset for this example: 

```python
data = make_dataset(df)
```

Now that we have the dataset, we want to split our data into two different parts: training and validation. Majority of the data will be used for training, since that's what we need to create a proper model! The validation data will be used as new pieces of data that the model hasn't seen to see how it responds at a certain round of training. For this example, we'll have 80% training data and 20% validation data overall. Since our dataset is quite large, we'll compile it into batches of 20 to make it more efficient. All together, we can split it up like so: 

```python
train_size = int(0.8*len(data))

train = data.take(train_size).batch(20)
val = data.skip(train_size).batch(20)
```

We now have data to create a model with! 

## Model Creation

Let's create three different models to see which analyzes the articles the best! One will use the article's title, one will use the article's text, and our final one will use both the title and text. To do so, we'll use the following process:
1. Create input that Tensorflow can interpret
2. Process the input information and vectorize it so that the algorithm can understand it 
3. Create each layer of the fake news identifier, which will ultimately become our model
4. Compile the model 
5. Train the model using our data from before

With this process in mind, let's create our first model, using only the article's title!

### Model 1 - Article Title

Let's follow the process we outlined above! To do so, let's first create input that Tensorflow understands. To do so, we'll use the function `keras.Input()` from Tensorflow. We can use it like so, indicating the shape, the data, and the datatype of the input: 

```python
title_input = keras.Input(
    shape = (1,),
    name = "title",
    dtype = "string"
)
```

Next, we'll need to process this information and vectorize it in a way that an algorithm can understand. Computers can understand numbers very clearly, so let's vectorize the strings so that each string is associated with a number instead. First, we'll need to standardize the strings. This involves making it all lowercase and taking out the punctuation. So, let's create a function executing just that: 

```python
def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 
```

With this function, we can create a layer in our algorithm just for vectorizing the input data. We'll use `TextVectorization()` from Tensorflow to do so. This layer from Tensorflow interprets text into numbers that will be understandable for other layers. For our purposes, we will only consider the 2000 most popular words within our dataset, and not specifically analyze every word. Afterwards, we'll want to analyze the entire title of the article, and not just the first word, so we'll need to use `adapt()` and `map()` to execute this. All together, creating this layer will look like so: 

```python
size_vocabulary = 2000

vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

#get the whole title out with lambda function
vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
```

This was the hardest part! With that out of the way, we can begin adding more layers. The other layers we'll use are the following: 
1. `Embedding` to see what words are associated with fake news. We'll use 10 dimensions of words to analyze. 
2. `Dropout` to deal with overfitting. This ensures the model isn't overly complex.
3. `GlobalAveragePooling1D` to take the average over the entire title. This makes our original 10 dimensional word to a 1D vector
4. `Dense` is the most basic layer, which will execute our basic machine learning algorithm!

We'll use all of these together to get all the features like so: 

```python
title_features = vectorize_layer(title_input)
title_features = layers.Embedding(size_vocabulary, output_dim = 10, name = "embedding_title")(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)

output_title = layers.Dense(2, name = "fake")(title_features)
```

With all of our layers, we can now construct our model! Let's use the function `keras.Model()` to do so:

```python
model_title = keras.Model(
    inputs = title_input,
    outputs = output_title
)
```

After constructing the model itself, we need to compile and train the model to create the fake news identifier! We'll use `compile()` and `fit()` to execute this. We'll use 50 epochs for this model, which means that we'll compile through the training data 50 times to train our model. It will look like so: 

```python
#compile the model
model_title.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics = ['accuracy'])
              
#train the model
history = model_title.fit(train, validation_data = val, epochs = 50)
```
```
Epoch 1/50

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:595: UserWarning:

Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.

898/898 [==============================] - 4s 4ms/step - loss: 0.6798 - accuracy: 0.5567 - val_loss: 0.3621 - val_accuracy: 0.9334
Epoch 2/50
898/898 [==============================] - 3s 4ms/step - loss: 0.2547 - accuracy: 0.9372 - val_loss: 0.1079 - val_accuracy: 0.9699
Epoch 3/50
898/898 [==============================] - 3s 4ms/step - loss: 0.1077 - accuracy: 0.9639 - val_loss: 0.0752 - val_accuracy: 0.9782
.....
Epoch 48/50
898/898 [==============================] - 3s 4ms/step - loss: 0.0227 - accuracy: 0.9913 - val_loss: 0.1506 - val_accuracy: 0.9559
Epoch 49/50
898/898 [==============================] - 3s 4ms/step - loss: 0.0227 - accuracy: 0.9909 - val_loss: 0.2045 - val_accuracy: 0.9396
Epoch 50/50
898/898 [==============================] - 3s 4ms/step - loss: 0.0210 - accuracy: 0.9917 - val_loss: 0.2023 - val_accuracy: 0.9405
```

The output actually shows each level, but I've shortened it here to make it easier to interpret at first glance. We want to look at our accuracy and loss levels. Over time, we see our loss value decrease and our accuracy incraese, which means we're on the right track! We can see that during our last epoch, we obtain a validation accuracy of 0.9405, or 94.05%, which means our model is pretty good! This accuracy denotes the accuracy of our model using the validation data we partitioned off earlier. Now, our title model is complete! 

### Model 2 - Article's Text

We'll be following the same process for analyzing the article's text for a model! So, lets create our input using the same function as Model 1:

```python
text_input = keras.Input(
    shape = (1,),
    name = "text",
    dtype = "string"
)
```

We'll also be using the same vectorization and the same layers as before for the same reasons. Ultimately, we're analyzing text, so the overall process is very similar. We'll use the same `standardization()` function and create all the layers of the model like so: 

```python
vectorize_layer.adapt(train.map(lambda x, y: x["text"]))

text_features = vectorize_layer(text_input)
text_features = layers.Embedding(size_vocabulary, output_dim = 10, name = "embedding_text")(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)

output_text = layers.Dense(2, name = "fake")(text_features)
```

Now that we have our layers, we can now construct our model with `keras.Model()` like so: 

```python
model_text = keras.Model(
    inputs = text_input,
    outputs = output_text
)
```

With this construction, let's compile and train our model, like we did for Model 1: 

```python
#compile the model
model_text.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics = ['accuracy'])
              
#train the model
history_text = model_text.fit(train, validation_data = val, epochs = 50)
```
```
Epoch 1/50

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:595: UserWarning:

Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.

898/898 [==============================] - 6s 6ms/step - loss: 0.5357 - accuracy: 0.7398 - val_loss: 0.1411 - val_accuracy: 0.9635
Epoch 2/50
898/898 [==============================] - 5s 5ms/step - loss: 0.1350 - accuracy: 0.9636 - val_loss: 0.1047 - val_accuracy: 0.9666
Epoch 3/50
898/898 [==============================] - 5s 5ms/step - loss: 0.0960 - accuracy: 0.9734 - val_loss: 0.0937 - val_accuracy: 0.9704
.....
Epoch 48/50
898/898 [==============================] - 5s 6ms/step - loss: 0.0079 - accuracy: 0.9964 - val_loss: 0.2093 - val_accuracy: 0.9728
Epoch 49/50
898/898 [==============================] - 5s 5ms/step - loss: 0.0078 - accuracy: 0.9967 - val_loss: 0.1982 - val_accuracy: 0.9757
Epoch 50/50
898/898 [==============================] - 5s 6ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.2087 - val_accuracy: 0.9744
```

Again, let's look at our accuracy and loss function over time! We can see our loss function overall decreases, and our accuracy increases, which is great! After the last epoch, we have a validation accuracy of 0.9744, or 97.44%, which is good improvement from our first model! Now, our text model is complete!

### Model 3 - Title and Text

For this final model, we'll be analyzing the article's title and text to determine if it contains false content. Since we've already created the models for analyzing title and text separately, this model will be quite simple! We just need to combine these two models, add a few `Dense` layers, and we'll be good to go! To combine the two models, we'll use `layers.contatenate()`, then create our `Dense` layers like the previous examples like so:

```python
main = layers.concatenate([title_features, text_features], axis = 1)
main = layers.Dense(32, activation = 'relu')(main)
output = layers.Dense(2, name = "fake")(main)
```

With this, our layers are all complete! Now, we can simply create our model like before: 

```python
model = keras.Model(
    inputs = [title_input, text_input],
    outputs = output
)
```

And that's it! To see exactly what our model is doing, let's use a handy function called `keras.utils.plot_model()`, which will show us the exact relationship between our input, the output, and all of the layers: 

```python
keras.utils.plot_model(model)
```
![hw3-flow.png]({{ site.baseurl }}/images/hw3-flow.png)

This diagram clearly shows how we end up reaching our output goal with the two inputs! Now, we can compile and train our model like before: 

```python
#compile the model
model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics = ['accuracy'])

#train the model
history = model.fit(train, validation_data = val, epochs = 50)
```
```
Epoch 1/50
898/898 [==============================] - 6s 7ms/step - loss: 0.0114 - accuracy: 0.9977 - val_loss: 0.1351 - val_accuracy: 0.9748
Epoch 2/50
898/898 [==============================] - 6s 7ms/step - loss: 0.0095 - accuracy: 0.9965 - val_loss: 0.1617 - val_accuracy: 0.9731
Epoch 3/50
898/898 [==============================] - 6s 7ms/step - loss: 0.0081 - accuracy: 0.9973 - val_loss: 0.1474 - val_accuracy: 0.9768
.....
Epoch 48/50
898/898 [==============================] - 6s 7ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.1266 - val_accuracy: 0.9842
Epoch 49/50
898/898 [==============================] - 6s 7ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.1283 - val_accuracy: 0.9815
Epoch 50/50
898/898 [==============================] - 6s 7ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.1306 - val_accuracy: 0.9826
```
With this model, let's analyze our loss function and accuracy again. Our loss function decreases over time, and our validation accuracy reaches 0.9826 or 98.26% after our last epoch. This is great! Our model works wonderfully in identifying fake news in articles. We have now successfuly created three different models for identifying fake news. 

Comparing the performance between all three methods, we can see that **Model 3**, which analyzes both the title and the text of an article, is best at detecting fake news. Second place would be Model 2, which analyzes only the text of an article, and last place would be Model 3, which analyzes the title of an article. Therefore, algorithms detecting fake news should use both resources if they are available! However, all three models achieved an accuracy rate of at least 94%, which is pretty good overall! So, if only the title of an article is available, it could still be a pretty accurate model. But, in an ideal case, we should use **Model 3** with both title and text for the best results!

## Model Evaluation

Now that we've determined that Model 3 is our best model, let's test it on some new data! For this, let's read a new csv file, use `make_dataset()` to make it a data type that Tensorflow can understand, and evaluate how accurate our model is. To evaluate, we'll use the `evaluate` function from Tensorflow:

```python
#model evaluation

test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
df_test = pd.read_csv(test_url)
data_test = make_dataset(df_test)

model.evaluate(data_test)
```
```
22449/22449 [==============================] - 35s 2ms/step - loss: 0.1424 - accuracy: 0.9815
[0.14241760969161987, 0.9815136790275574]
```

This output means that our loss function has a value of 0.1424 and our accuracy is 0.9815, or 98.15%, which is excellent! Our model works on a totally new dataset. With this model, we'd detect fake news around 98% of the time. 

## Embedding Visualization

In our model, we embedded two different 
