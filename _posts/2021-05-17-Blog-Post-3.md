---
layout: post
title: Blog Post 3 - Analyzing Fake News with Tensorflow
---

This blog post will explain how create a model to identify fake news using Tensorflow. 

## Acquire Training Data

First, this model uses lots of modules, so let's import them and explain their use:

```python
import numpy as np
import pandas as pd
import tensorflow as tf
import re #regular expressions
import string

from tensorflow.keras import layers #creating layers
from tensorflow.keras import losses #loss functions
from tensorflow import keras

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

# for embedding viz
import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"
```

Overall, we have a little bit to unpack here, but we'll mainly be using the following modules for various reasons: 
- `numpy`: to manipulate arrays in a convenient way
- `pandas`: to create dataframes
- `tensorflow`: to implement machine learning techniques
    - `layers` is for creating layers in our algorithm
    - `losses` is for loss functions
- `re`: for creating regular expressions for string recognition 
- `string`: for manipulating strings
- `plotly`: for ploting visualizations of our data

With these modules, let's first acquire our training data! For this project, we want to ultimately create a model that can accurately determine if a news article is fake or not. To do so, we will need some data to train our model. This training data contains information about the title of the article, text from the article, and whether it is fake or not. Let's use `pd.read_csv()` to first obtain this data: 

```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"

df = pd.read_csv(train_url)
df
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>22444</th>
      <td>10709</td>
      <td>ALARMING: NSA Refuses to Release Clinton-Lynch...</td>
      <td>If Clinton and Lynch just talked about grandki...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22445</th>
      <td>8731</td>
      <td>Can Pence's vow not to sling mud survive a Tru...</td>
      <td>() - In 1990, during a close and bitter congre...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22446</th>
      <td>4733</td>
      <td>Watch Trump Campaign Try To Spin Their Way Ou...</td>
      <td>A new ad by the Hillary Clinton SuperPac Prior...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22447</th>
      <td>3993</td>
      <td>Trump celebrates first 100 days as president, ...</td>
      <td>HARRISBURG, Pa.U.S. President Donald Trump hit...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22448</th>
      <td>12896</td>
      <td>TRUMP SUPPORTERS REACT TO DEBATE: “Clinton New...</td>
      <td>MELBOURNE, FL is a town with a population of 7...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>22449 rows × 4 columns</p>
</div>

In this dataframe, the `fake` column will have `0` represent if an article is true, and `1` if the article contains fake news. Now, we have lots of data to train our model with! 

## Make a Dataset

Now that we have the data that we want to analyze, we'll need to first create a dataset that Tensorflow will be able to interpret. To do so, let's create a function `make_dataset()` that will both clean our data and compile it into this dataset. 

For cleaning our data, we want to remove all *stopwords* from it, or words that are usually uninformative, such as "the" or "and." To recognize these words, we'll use the list of stopwords from the module `ntlk`. Essentially, we want to go through every word in the title and text of each article, remove it if it's a stopword, then create our dataset with these cleaned data. 

We want to have the model observe the title and text of the article, then predict if the article has fake information in it or not. Therefore, we want to create a dataset in the form like so: `(input, output)`. We'll use a tuple of dictionaries to do this, so the "title" of an article is associated with the `title` column of the dataframe `df` from before. All together, the code should look like so: 

```python
#import list of stopwords
import nltk
nltk.download('stopwords')
stop = stopwords.words('english')

def make_dataset(df):
  #remove stopwords
  df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

  #create dataset
  data = tf.data.Dataset.from_tensor_slices(
      (
        {
            "title" : df[['title']],
            "text" : df[['text']]
        },
       {
            "fake" : df[['fake']]   
       }
      )
  )
  data.batch(100)
  return data
```

Great! Now we have a function that takes in a `pandas` dataframe and returns a Tensorflow dataset that we can use for our machine learning algorithm. With this function, let's create our dataset for this example: 

```python
data = make_dataset(df)
```

Now that we have the dataset, we want to split our data into two different parts: training and validation. Majority of the data will be used for training, since that's what we need to create a proper model! The validation data will be used as new pieces of data that the model hasn't seen to see how it responds at a certain round of training. For this example, we'll have 80% training data and 20% validation data overall. Since our dataset is quite large, we'll compile it into batches of 20 to make it more efficient. All together, we can split it up like so: 

```python
train_size = int(0.8*len(data))

train = data.take(train_size).batch(20)
val = data.skip(train_size).batch(20)
```

We now have data to create a model with! 

## Model Creation

Let's create three different models to see which analyzes the articles the best! One will use the article's title, one will use the article's text, and our final one will use both the title and text. To do so, we'll use the following process:
1. Create input that Tensorflow can interpret
2. Process the input information and vectorize it so that the algorithm can understand it 
3. Create each layer of the fake news identifier, which will ultimately become our model
4. Compile the model 
5. Train the model using our data from before

With this process in mind, let's create our first model, using only the article's title!

### Model 1 - Article Title

Let's follow the process we outlined above! To do so, let's first create input that Tensorflow understands. To do so, we'll use the function `keras.Input()` from Tensorflow. We can use it like so, indicating the shape, the data, and the datatype of the input: 

```python
title_input = keras.Input(
    shape = (1,),
    name = "title",
    dtype = "string"
)
```

Next, we'll need to process this information and vectorize it in a way that an algorithm can understand. Computers can understand numbers very clearly, so let's vectorize the strings so that each string is associated with a number instead. First, we'll need to standardize the strings. This involves making it all lowercase and taking out the punctuation. So, let's create a function executing just that: 

```python
def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 
```

With this function, we can create a layer in our algorithm just for vectorizing the input data. We'll use `TextVectorization()` from Tensorflow to do so. This layer from Tensorflow interprets text into numbers that will be understandable for other layers. For our purposes, we will only consider the 2000 most popular words within our dataset, and not specifically analyze every word. Afterwards, we'll want to analyze the entire title of the article, and not just the first word, so we'll need to use `adapt()` and `map()` to execute this. All together, creating this layer will look like so: 

```python
size_vocabulary = 2000

vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

#get the whole title out with lambda function
vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
```

This was the hardest part! With that out of the way, we can begin adding more layers. The other layers we'll use are the following: 
1. `Embedding` to see what words are associated with fake news. We'll use 10 dimensions of words to analyze. 
2. `Dropout` to deal with overfitting. This ensures the model isn't overly complex.
3. `GlobalAveragePooling1D` to take the average over the entire title. This makes our original 10 dimensional word to a 1D vector
4. `Dense` is the most basic layer, which will execute our basic machine learning algorithm!

We'll use all of these together to get all the features like so: 

```python
title_features = vectorize_layer(title_input)
title_features = layers.Embedding(size_vocabulary, output_dim = 10, name = "embedding_title")(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)

output_title = layers.Dense(2, name = "fake")(title_features)
```

With all of our layers, we can now construct our model! Let's use the function `keras.Model()` to do so:

```python
model_title = keras.Model(
    inputs = title_input,
    outputs = output_title
)
```

After constructing the model itself, we need to compile and train the model to create the fake news identifier! We'll use `compile()` and `fit()` to execute this. We'll use 50 epochs for this model, which means that we'll compile through the training data 50 times to train our model. It will look like so: 

```python
#compile the model
model_title.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics = ['accuracy'])
              
#train the model
history = model_title.fit(train, validation_data = val, epochs = 50)
```
```
Epoch 1/50

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:595: UserWarning:

Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.

898/898 [==============================] - 4s 4ms/step - loss: 0.6798 - accuracy: 0.5567 - val_loss: 0.3621 - val_accuracy: 0.9334
Epoch 2/50
898/898 [==============================] - 3s 4ms/step - loss: 0.2547 - accuracy: 0.9372 - val_loss: 0.1079 - val_accuracy: 0.9699
Epoch 3/50
898/898 [==============================] - 3s 4ms/step - loss: 0.1077 - accuracy: 0.9639 - val_loss: 0.0752 - val_accuracy: 0.9782
Epoch 4/50
898/898 [==============================] - 3s 4ms/step - loss: 0.0795 - accuracy: 0.9729 - val_loss: 0.0639 - val_accuracy: 0.9795
.....
Epoch 48/50
898/898 [==============================] - 3s 4ms/step - loss: 0.0227 - accuracy: 0.9913 - val_loss: 0.1506 - val_accuracy: 0.9559
Epoch 49/50
898/898 [==============================] - 3s 4ms/step - loss: 0.0227 - accuracy: 0.9909 - val_loss: 0.2045 - val_accuracy: 0.9396
Epoch 50/50
898/898 [==============================] - 3s 4ms/step - loss: 0.0210 - accuracy: 0.9917 - val_loss: 0.2023 - val_accuracy: 0.9405
```

The output actually shows each level, but I've shortened it here to make it easier to interpret at first glance. We want to look at our accuracy and loss levels. Overtime, we see our loss value decrease and our accuracy incraese, which means we're on the right track! We can see that during our last epoch, we obtain a validation accuracy of 0.9405, or 94.05%, which means our model is pretty good! This accuracy denotes the accuracy of our model using the validation data we partitioned off earlier. Now, our title model is complete! 

### Model 2 - 
